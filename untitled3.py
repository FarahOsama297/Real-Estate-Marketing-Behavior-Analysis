# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1teuk8gJi8bldAJyaQBcNgh47JuvkWkUP
"""

import pandas as pd

df = df = pd.read_csv("practice_dataset_240_modified_v3.csv")
df.head()

duplicate_rows = df[df.duplicated(keep=False)]
print(f"Total duplicate rows (showing all instances of duplicates): {len(duplicate_rows)}")
display(duplicate_rows)

if "Timestamp" in df.columns:
    df = df.drop(columns=["Timestamp"])
if "Education Level" in df.columns:
    df = df.drop(columns=["Education Level"])

# ----- 2. REMOVE DUPLICATES -----
df = df.drop_duplicates()

# ----- 3. DEFINE MAPPINGS -----
likert_map = {
    "Strongly Disagree": 1,
    "Disagree": 2,
    "Neutral": 3,
    "Agree": 4,
    "Strongly Agree": 5
}

yes_no_map = {"Yes": 1, "No": 0}
gender_map = {"Male": 1, "Female": 0}
Education_Level = {"Bachelor?s Degree": 0, "Master?s Degree": 1}

# ----- 4. APPLY MAPPINGS (EXCEPT AGE) -----
for col in df.columns:
    if col != "Age":  # do NOT map Age
        df[col] = df[col].replace(likert_map)
        df[col] = df[col].replace(yes_no_map)
        df[col] = df[col].replace(gender_map)
        df[col] = df[col].replace(Education_Level)

# Create a mapping from long column names to short meaningful ones:
rename_map = {
    #LookingOrPurchasedHome
    "Have you purchased or are you currently looking to purchase a home?": "LookingOrPurchased",
    # Section A — Scarcity Messaging
    "Scarcity Messaging [Marketing messages showing limited real estate availability make me feel the opportunity may end soon.]":
        "Scarcity_MissingOutFearing",
    "Scarcity Messaging [When a property or unit is advertised as scarce, I feel more motivated to consider it.]":
        "Scarcity_MotivationIncrease",
    "Scarcity Messaging [Messages stating that few units remain increase my interest in the offer.]":
        "Scarcity_InterestBoosting",
    "Scarcity Messaging [Seeing a property promoted as “limited” makes me think it will sell out quickly.]":
        "Scarcity_SellOutExpecting",

    # Section B — Perceived Urgency
    "Perceived Urgency [Time-sensitive real estate offers make me feel pressured to make a faster decision.]":
        "Urgency_PressureFeeling",
    "Perceived Urgency [I feel I might lose the opportunity of buying a property if I do not act quickly.]":
        "Urgency_LossFear",
    "Perceived Urgency [Marketing messages with deadlines in the real estate industry increase my sense of urgency.]":
        "Urgency_DeadlineEffect",
    "Perceived Urgency [Urgent real estate promotions make me accelerate my decision-making process.]":
        "Urgency_FastDecisionMaking",

    # Section C — Purchase Intention
    "Purchase Intention [I am likely to purchase a property when an offer appears favorable.]":
        "Intent_FavorableOffer",
    "Purchase Intention [If I believe the opportunity of buying a desired property might disappear, I am more willing to commit financially.]":
        "Intent_MissingOutWillingness",
    "Purchase Intention [I am willing to make a booking or reservation if a “good” offer is available.]":
        "Intent_BookingWillingness",
    "Purchase Intention [Scarcity or urgency messages in real estate increase my intention to take purchase-related action.]":
        "Intent_ScarcityDriven",
}

# Apply renaming
df = df.rename(columns=rename_map)

# ----- 5. SAVE CLEANED FILE -----
df.to_csv("cleaned_practice_dataset_240_modified_v3.csv", index=False)

print("Cleaning complete. Saved as cleaned_real_estate_survey.csv")
print(df.head())

df.describe()

groups = {
    "Scarcity": [
        "Scarcity_MissingOutFearing",
        "Scarcity_MotivationIncrease",
        "Scarcity_InterestBoosting",
        "Scarcity_SellOutExpecting"
    ],

    "Urgency": [
        "Urgency_PressureFeeling",
        "Urgency_LossFear",
        "Urgency_DeadlineEffect",
        "Urgency_FastDecisionMaking"
    ],

    "Intention": [
        "Intent_FavorableOffer",
        "Intent_MissingOutWillingness",
        "Intent_BookingWillingness",
        "Intent_ScarcityDriven"
    ]
}

import numpy as np

def cronbach_alpha(df_group):
    df_corr = df_group.corr()
    N = df_group.shape[1]
    avg_corr = df_corr.where(np.triu(np.ones(df_corr.shape), k=1).astype(bool)).mean().mean()
    alpha = (N * avg_corr) / (1 + (N - 1) * avg_corr)
    return alpha

alpha_scarcity = cronbach_alpha(df[groups["Scarcity"]])
alpha_urgency = cronbach_alpha(df[groups["Urgency"]])
alpha_intention = cronbach_alpha(df[groups["Intention"]])

alpha_scarcity, alpha_urgency, alpha_intention

df.nunique()

df.corr()

df["Scarcity_score"] = df[groups["Scarcity"]].mean(axis=1)
df["Urgency_score"] = df[groups["Urgency"]].mean(axis=1)
df["Intention_score"] = df[groups["Intention"]].mean(axis=1)

group_corr = df[["Scarcity_score", "Urgency_score", "Intention_score"]].corr()
group_corr

independent_variables = [
    "Scarcity_score",
    "Urgency_score",
    "Age",
    "Gender",
    "Education"
]

dependent_variable = "Intention_score"

print(f"Independent Variables: {independent_variables}")
print(f"Dependent Variable: {dependent_variable}")

"""**Reasoning**:
Now that the dependent and independent variables are defined, I will split the dataset into training and testing sets to prepare for the regression analysis. This ensures that the model can be evaluated on unseen data.


"""

from sklearn.model_selection import train_test_split

X = df[independent_variables]
y = df[dependent_variable]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("Data split into training and testing sets.")
print(f"X_train shape: {X_train.shape}")
print(f"X_test shape: {X_test.shape}")
print(f"y_train shape: {y_train.shape}")
print(f"y_test shape: {y_test.shape}")

from sklearn.linear_model import LinearRegression

# Initialize the Linear Regression model
model = LinearRegression()

# Train the model on the training data
model.fit(X_train, y_train)

print("Linear Regression model trained successfully.")
print(f"Model coefficients: {model.coef_}")
print(f"Model intercept: {model.intercept_}")

from sklearn.metrics import mean_squared_error, r2_score

# Make predictions on the test set
y_pred = model.predict(X_test)

# Evaluate the model's performance
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Model performance on the test set:")
print(f"Mean Squared Error (MSE): {mse:.2f}")
print(f"R-squared (R2): {r2:.2f}")

from sklearn.preprocessing import StandardScaler

# Initialize the StandardScaler
scaler = StandardScaler()

# Fit the scaler on the training data and transform both training and test data
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print("Independent variables scaled successfully.")
print(f"X_train_scaled shape: {X_train_scaled.shape}")
print(f"X_test_scaled shape: {X_test_scaled.shape}")

from sklearn.ensemble import RandomForestRegressor

# Initialize the RandomForestRegressor model
# Using random_state for reproducibility
rf_model = RandomForestRegressor(random_state=42)

# Train the model on the scaled training data
rf_model.fit(X_train_scaled, y_train)

print("RandomForestRegressor model trained successfully.")

from sklearn.metrics import mean_squared_error, r2_score

# Make predictions on the scaled test set
y_pred_rf = rf_model.predict(X_test_scaled)

# Evaluate the RandomForestRegressor model's performance
mse_rf = mean_squared_error(y_test, y_pred_rf)
r2_rf = r2_score(y_test, y_pred_rf)

print(f"RandomForestRegressor Model performance on the test set:")
print(f"Mean Squared Error (MSE): {mse_rf:.2f}")
print(f"R-squared (R2): {r2_rf:.2f}")

from sklearn.ensemble import GradientBoostingRegressor

# Initialize the GradientBoostingRegressor model
# Using random_state for reproducibility
gbr_model = GradientBoostingRegressor(random_state=42)

# Train the model on the scaled training data
gbr_model.fit(X_train_scaled, y_train)

print("GradientBoostingRegressor model trained successfully.")

from sklearn.metrics import mean_squared_error, r2_score

# Make predictions on the scaled test set
y_pred_gbr = gbr_model.predict(X_test_scaled)

# Evaluate the GradientBoostingRegressor model's performance
mse_gbr = mean_squared_error(y_test, y_pred_gbr)
r2_gbr = r2_score(y_test, y_pred_gbr)

print(f"GradientBoostingRegressor Model performance on the test set:")
print(f"Mean Squared Error (MSE): {mse_gbr:.2f}")
print(f"R-squared (R2): {r2_gbr:.2f}")

import seaborn as sns
import matplotlib.pyplot as plt

# Create a heatmap of the group_corr correlation matrix
plt.figure(figsize=(8, 6))
sns.heatmap(group_corr, annot=True, cmap='coolwarm', fmt=".2f", linewidths=.5)
plt.title('Correlation Heatmap of Scarcity, Urgency, and Intention Scores')
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Calculate the mean score for each group
mean_scores = {
    'Scarcity': df['Scarcity_score'].mean(),
    'Urgency': df['Urgency_score'].mean(),
    'Intention': df['Intention_score'].mean()
}

# Convert to a DataFrame for easier plotting
mean_scores_df = pd.DataFrame(mean_scores.items(), columns=['Group', 'Mean Score'])

# Create the bar chart
plt.figure(figsize=(8, 6))
sns.barplot(x='Group', y='Mean Score', data=mean_scores_df, palette='pastel')
plt.title('Mean Scores for Scarcity, Urgency, and Intention Groups')
plt.xlabel('Group')
plt.ylabel('Mean Score')
plt.ylim(0, 5) # Assuming Likert scale from 1 to 5
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# List of Likert scale question columns
likert_columns = [
    "Scarcity_MissingOutFearing",
    "Scarcity_MotivationIncrease",
    "Scarcity_InterestBoosting",
    "Scarcity_SellOutExpecting",
    "Urgency_PressureFeeling",
    "Urgency_LossFear",
    "Urgency_DeadlineEffect",
    "Urgency_FastDecisionMaking",
    "Intent_FavorableOffer",
    "Intent_MissingOutWillingness",
    "Intent_BookingWillingness",
    "Intent_ScarcityDriven"
]

# Create a DataFrame to store value counts for each question
data_for_plot = []

for col in likert_columns:
    # Get value counts for the current column
    counts = df[col].value_counts().sort_index()
    for likert_value in range(1, 6): # Likert scale values 1 to 5
        count = counts.get(likert_value, 0) # Get count, 0 if not present
        data_for_plot.append({'Question': col, 'Response': likert_value, 'Count': count})

# Convert the list of dictionaries to a DataFrame
plot_df = pd.DataFrame(data_for_plot)

# Create the stacked bar chart
plt.figure(figsize=(15, 8))
sns.barplot(x='Question', y='Count', hue='Response', data=plot_df, palette='viridis', dodge=False)
plt.title('Distribution of Likert Scale Responses for Each Question')
plt.xlabel('Question')
plt.ylabel('Number of Respondents')
plt.xticks(rotation=90, ha='right')
plt.legend(title='Likert Response (1-5)', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

# Prepare data: Get the mean scores for the three groups
scores = [
    df['Scarcity_score'].mean(),
    df['Urgency_score'].mean(),
    df['Intention_score'].mean()
]
labels = ['Scarcity_score', 'Urgency_score', 'Intention_score']

# Number of variables
num_vars = len(labels)

# Calculate angle for each axis
angles = np.linspace(0, 2 * np.pi, num_vars, endpoint=False).tolist()

# The plot is a circle, so the first value must be repeated to close the circle
scores = np.concatenate((scores, [scores[0]]))
angles = np.concatenate((angles, [angles[0]]))

# Set the max value for the radar chart (Likert scale max is 5)
max_value = 5

# Create the radar chart
fig, ax = plt.subplots(figsize=(6, 6), subplot_kw=dict(polar=True))

# Plot data
ax.plot(angles, scores, linewidth=1, linestyle='solid', label='Mean Score')
ax.fill(angles, scores, 'b', alpha=0.1)

# Draw axis lines and labels
ax.set_theta_offset(np.pi / 2)
ax.set_theta_direction(-1)
ax.set_xticks(angles[:-1])
ax.set_xticklabels(labels)

# Draw ylabels
ax.set_rlabel_position(angles[0])
plt.yticks(np.arange(0, max_value + 1, 1), color="grey", size=8)
plt.ylim(0, max_value)

ax.set_title('Radar Chart of Mean Scores', loc='left', y=1.0, ha='left')
plt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(10, 6))
sns.histplot(df['Age'], bins=10, kde=True)
plt.title('Distribution of Age')
plt.xlabel('Age')
plt.ylabel('Frequency')
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# Count the occurrences of each gender
gender_counts = df['Gender'].value_counts().reset_index()
gender_counts.columns = ['Gender', 'Count']

# Map numerical gender back to labels for better readability in the plot
gender_labels = {1: 'Male', 0: 'Female'}
gender_counts['Gender_Label'] = gender_counts['Gender'].map(gender_labels)

# Create the bar chart
plt.figure(figsize=(7, 5))
sns.barplot(x='Gender_Label', y='Count', data=gender_counts, palette='viridis')
plt.title('Distribution of Gender')
plt.xlabel('Gender')
plt.ylabel('Count')
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Create a DataFrame for Cronbach's Alpha values
alpha_data = {
    'Group': ['Scarcity', 'Urgency', 'Intention'],
    'Cronbach_Alpha': [alpha_scarcity, alpha_urgency, alpha_intention]
}
alpha_df = pd.DataFrame(alpha_data)

# Create the bar chart
plt.figure(figsize=(8, 6))
sns.barplot(x='Group', y='Cronbach_Alpha', data=alpha_df, palette='magma')
plt.title('Cronbach\'s Alpha for Scarcity, Urgency, and Intention Scales')
plt.xlabel('Group')
plt.ylabel('Cronbach\'s Alpha Value')
plt.ylim(0.7, 1.0) # Set appropriate y-axis limits for reliability scores
plt.axhline(y=0.7, color='r', linestyle='--', label='Acceptable Threshold (0.7)')
plt.legend()
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Prepare data for boxplot by melting the relevant columns
boxplot_data = df[['Scarcity_score', 'Urgency_score', 'Intention_score']].melt(var_name='Group', value_name='Score')

# Create the boxplot
plt.figure(figsize=(10, 6))
sns.boxplot(x='Group', y='Score', data=boxplot_data, palette='viridis')
plt.title('Boxplot of Scarcity, Urgency, and Intention Scores')
plt.xlabel('Group')
plt.ylabel('Score (1-5)')
plt.ylim(0, 5.5) # Set y-axis limits from 0 to slightly above 5 for Likert scale
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

print("--- Assessing Validity ---")

# 1. Construct Validity (Convergent & Discriminant Validity)
# Convergent validity: Measures that theoretically should be related are indeed related.
# Discriminant validity: Measures that theoretically should not be related are distinct.
# The correlation matrix below shows the relationships between the composite scores for Scarcity, Urgency, and Intention.
# Strong positive correlations between these constructs suggest convergent validity, while correlations less than 1 indicate distinctness, supporting discriminant validity.
print("\nCorrelation Matrix of Composite Scores (Construct Validity):")
print(group_corr)

# 2. Predictive Validity
# Predictive validity assesses how well the model predicts the outcome variable (Intention_score).
# The R-squared value from the Linear Regression model indicates the proportion of variance in the dependent variable that is predictable from the independent variables.
print("\nLinear Regression Model R-squared (Predictive Validity):")
print(f"R-squared: {r2:.2f}")

print("\nInterpretation:\n- The correlation matrix shows positive relationships between Scarcity, Urgency, and Intention, supporting convergent validity. The fact that these correlations are not 1 indicates some level of discriminant validity, suggesting they are related but distinct constructs.\n- The R-squared value of 0.56 for the Linear Regression model indicates that 56% of the variance in Purchase Intention can be explained by Scarcity, Urgency, Age, Gender, and Education, demonstrating a moderate level of predictive validity.")

from scipy import stats

# Separate the data into two groups based on Gender
intention_male = df[df['Gender'] == 1]['Intention_score']
intention_female = df[df['Gender'] == 0]['Intention_score']

# Perform independent samples t-test
t_statistic, p_value = stats.ttest_ind(intention_male, intention_female)

print(f"--- Independent Samples t-test for Intention Score by Gender ---")
print(f"Mean Intention Score (Male): {intention_male.mean():.2f}")
print(f"Mean Intention Score (Female): {intention_female.mean():.2f}")
print(f"T-statistic: {t_statistic:.2f}")
print(f"P-value: {p_value:.3f}")

if p_value < 0.05:
    print("\nInterpretation: Since the p-value is less than 0.05, there is a statistically significant difference in Intention Scores between males and females.")
else:
    print("\nInterpretation: Since the p-value is greater than 0.05, there is no statistically significant difference in Intention Scores between males and females.")

from scipy import stats

# Get the unique education levels
education_levels = df['Education'].unique()

# Create a list of Intention_score for each education group
education_groups = [df['Intention_score'][df['Education'] == level] for level in education_levels]

# Perform one-way ANOVA
f_statistic, p_value = stats.f_oneway(*education_groups)

print(f"--- One-Way ANOVA for Intention Score by Education Level ---")
print(f"F-statistic: {f_statistic:.2f}")
print(f"P-value: {p_value:.3f}")

if p_value < 0.05:
    print("\nInterpretation: Since the p-value is less than 0.05, there is a statistically significant difference in Intention Scores between different education levels.")
else:
    print("\nInterpretation: Since the p-value is greater than 0.05, there is no statistically significant difference in Intention Scores between different education levels.")

# Optionally, print mean intention scores per education level
print("\nMean Intention Score by Education Level:")
for level in education_levels:
    mean_score = df[df['Education'] == level]['Intention_score'].mean()
    if level == 0:
        print(f"  Bachelor's Degree (0): {mean_score:.2f}")
    elif level == 1:
        print(f"  Master's Degree (1): {mean_score:.2f}")